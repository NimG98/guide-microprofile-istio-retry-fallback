// INSTRUCTION: Please remove all comments that start INSTRUCTION prior to commit. Most comments should be removed, although not the copyright.
// INSTRUCTION: The copyright statement must appear at the top of the file
//
// Copyright (c) 2018, 2019 IBM Corporation and others.
// Licensed under Creative Commons Attribution-NoDerivatives
// 4.0 International (CC BY-ND 4.0)
//   https://creativecommons.org/licenses/by-nd/4.0/
//
// Contributors:
//     IBM Corporation
//
:projectid: microprofile-istio-retry
:page-layout: guide-multipane
:page-duration: 30 minutes
:page-releasedate: 2019-09-10
:page-description: Explore how to use Fault Tolerance with MicroProfile and Istio
:page-tags: ['Kubernetes']
:page-permalink: /guides/{projectid}
:page-related-guides: ['istio-intro', 'microprofile-fallback', `iguide-retry-timeout`]
:common-includes: https://raw.githubusercontent.com/OpenLiberty/guides-common/master
:source-highlighter: prettify
:page-seo-title: Retry and Fallback with MicroProfile and Istio Fault Tolerance
:page-seo-description: How to retry services and add fallback behaviour with MicroProfile and Istio Fault Tolerance
:guide-author: Open Liberty
= Building fault-tolerant microservices using Istio Retry with Microprofile Fallback

[.hidden]
NOTE: This repository contains the guide documentation source. To view the guide in published form, view it on the https://openliberty.io/guides/{projectid}.html[Open Liberty website].


Explore how to manage the impact of failures using MicroProfile and Istio Fault Tolerance by adding retry and fallback behavior to microservice dependencies.

:kube: Kubernetes
:istio: Istio
:win: WINDOWS
:mac: MAC
:linux: LINUX
:docker: Docker
:minikube: Minikube
:maven: Maven


// =================================================================================================
// Introduction
// =================================================================================================

== What you'll learn


You will learn how to combine https://microprofile.io/[MicroProfile^] Retry and Fallback policies with https://istio.io/[Istio^] Retry 
to make your microservices more resilient to common failures like network problems.

Microservices that are created using MicroProfile can be freely deployed in a service mesh architecture to reduce the complexity 
associated with microservice functionality. Istio is a service mesh, meaning that itâ€™s a platform for managing how microservices interact with each other and the outside world.
{istio} consists of a control plane and sidecars that are injected into application pods. The sidecars contain
the https://www.envoyproxy.io/[Envoy^] proxy. You can think of Envoy as a sidecar that intercepts
and controls all the HTTP and TCP traffic to and from your container.
If you would like to learn more about Istio, check out the https://openliberty.io/guides/istio-intro.html[Managing microservice traffic using Istio^] guide.

MicroProfile and Istio both provide simple and flexible solutions to build fault-tolerant microservices. 
Fault tolerance leverages different strategies to guide the execution and result of logic.
A few fault tolerance policies that MicroProfile can offer include Retry, Timeout, Circuit Breaker, Bulkhead, and Fallback.
There is some overlap that exists between MicroProfile and Istio Fault Tolerance, such as the Retry policy.
However, Istio does not offer any fallback capabilities.
To view the available fault tolerance policies in MicroProfile and Istio, refer to the 
https://www.eclipse.org/community/eclipse_newsletter/2018/september/MicroProfile_istio.php#faulttolerance[comparison between MicroProfile and Istio fault handling^].

Use the Retry policy to fail quickly and recover from brief intermittent issues. 
An application might experience these transient failures when a microservice is undeployed, a database is overloaded by queries, 
the network connection becomes unstable, or the site host has a brief downtime. In these cases, rather than failing quickly on these transient failures, 
the Retry policy provides another chance for the request to succeed. 
Simply retrying the request might be all you need to do to make it succeed.

Fallback offers an alternative result when an execution does not complete successfully.
You will use the `@Fallback` annotations from the MicroProfile Fault Tolerance specification to define criteria 
for when to provide an alternative solution for a failed execution.

You will create a microservice ecosystem demonstrating MicroProfile Fault Tolerance with Istio fault handling.
Both libraries can be enabled when you want your microservices to have a service mesh architecture with Istio, 
and use MicroProfile to provide the extra fault tolerance policies that do not exist within Istio.
However, microservices that have the same fault tolerance policy enabled by both MicroProfile and Istio will
have a behaviour that is not as expected. 

The application that you will be working with is an `inventory` service, which collects, stores, and returns the system properties. 
It uses the `system` service to retrieve the system properties for a particular host. 
You will add fault tolerance to the `inventory` service so that it reacts accordingly when the `system` service is unavailable.


// =================================================================================================
// Prerequisites
// =================================================================================================

include::{common-includes}/kube-prereq.adoc[]

// =================================================================================================
// Getting Started
// =================================================================================================

[role='command']
include::{common-includes}/gitclone.adoc[]

// no "try what you'll build" section in this guide since it would be too long due to all setup the user will have to do.

// =================================================================================================
// Preparing your cluster and deploying Istio
// =================================================================================================

== Preparing your cluster and deploying Istio

:minikube-start: minikube start --memory=8192 --cpus=4 --kubernetes-version=v1.13.0
:docker-desktop-description: Check your settings to ensure that you have an adequate amount of memory allocated to your Docker Desktop enviornment, 8GB is recommended but 4GB should be adequate if you don't have enough RAM.
:minikube-description: The memory flag allocates 8GB of memory to your Minikube cluster. If you don't have enough RAM, then 4GB should be adequate.
[role=command]
include::{common-includes}/kube-start.adoc[leveloffset=+1]

=== Deploying Istio

First, go to the https://github.com/istio/istio/releases/latest[{istio} release page^] and download the latest stable release. Extract the archive and navigate to the directory with the extracted files.

Next, deploy the {istio} custom resource definitions. Custom resource definitions allow {istio} to define custom {kube} resources that you can use in your resource definition files.

****
[system]#*{linux} | {mac}*#

[role=command]
```
for i in install/kubernetes/helm/istio-init/files/crd*yaml; do kubectl apply -f $i; done
```

[system]#*{win}*#
[role=command]
```
FOR %i in (install\kubernetes\helm\istio-init\files\crd*yaml) DO kubectl apply -f %i
```
****

Next, deploy {istio} resources to your cluster by running the `kubectl apply` command, which creates or updates
{kube} resources defined in a yaml file. This command deploys {istio}.

[role=command]
```
kubectl apply -f install/kubernetes/istio-demo.yaml
```

Verify that {istio} was successfully deployed. All the values in the `AVAILABLE` column will have a value of `1` after
the deployment is complete.

[role=command]
```
kubectl get deployments -n istio-system
```
 
Ensure that the {istio} deployments are all available before you continue. The deployments might take a few minutes to become available. If the deployments aren't available after a few minutes, then increase the amount of memory available to your {kube} cluster. On Docker Desktop, you can increase the memory from your {docker} preferences. On {minikube}, you can increase the memory using the `--memory` flag.
[source, role="no_copy"]
----
NAME                     DESIRED   CURRENT   UP-TO-DATE   AVAILABLE   AGE
grafana                  1         1         1            1           44s
istio-citadel            1         1         1            1           44s
istio-egressgateway      1         1         1            1           44s
istio-galley             1         1         1            1           44s
istio-ingressgateway     1         1         1            1           44s
istio-pilot              1         1         1            1           44s
istio-policy             1         1         1            1           44s
istio-sidecar-injector   1         1         1            1           44s
istio-telemetry          1         1         1            1           44s
istio-tracing            1         1         1            1           43s
prometheus               1         1         1            1           44s
servicegraph             1         1         1            1           44s
----

Finally, create the `istio-injection` label and set its value to `enabled`.

[role=command]
```
kubectl label namespace default istio-injection=enabled
```

Adding this label enables automatic {istio} sidecar injection. Automatic injection means that sidecars will
automatically be injected into your pods when you deploy your application. You don't need to perform
any additional steps for the sidecars to be injected.

// =================================================================================================
// Enabling MicroProfile fault tolerance
// =================================================================================================

== Enabling MicroProfile fault tolerance

Navigate to the `start` directory to begin.

The MicroProfile Fault Tolerance API was added as a dependency to your `inventory` service's [hotspot file=0]`pom.xml` file. Look for
the dependency with the [hotspot=mpFaultTolerance file=0]`mpFaultTolerance` artifact ID. Adding this dependency allows you to use the
fault tolerance policies in your microservices.

You can also find the [hotspot=mpFaultTolerance2 file=1]`mpFaultTolerance` feature in your inventory [hotspot file=1]`server.xml` server configuration,
which turns on MicroProfile Fault Tolerance capabilities in Open Liberty.

Next, the [hotspot file=2]`SystemClient.java` file
makes a request to the `system` service through the MicroProfile Rest Client API.
If you want to learn more about MicroProfile Rest Client,
you can follow the https://openliberty.io/guides/microprofile-rest-client.html[Consuming RESTful services with template interfaces^] guide.
To simulate that your `system` service is temporarily down, due to brief intermittent issues, any requests made to your `system` service 
have to wait for a long time. A [hotspot=timeout file=3]`Timeout` policy was applied to limit the time your `inventory` service's request 
to your `system` service waits to 2000 milliseconds. 
The `@Timeout` annotation helps by ending requests that have taken too long and are unlikely to return successfully.




pom.xml
[source, xml, linenums, role='code_column']
----
include::finish/inventory/pom.xml[]
----

server.xml
[source, xml, linenums, role='code_column']
----
include::finish/inventory/src/main/liberty/config/server.xml[]
----

SystemClient.java
[source, java, linenums, role='code_column']
----
include::finish/inventory/src/main/java/io/openliberty/guides/inventory/client/SystemClient.java[tags=**;!copyright]
----

InventoryResource.java
[source, java, linenums, role='code_column']
----
include::finish/inventory/src/main/java/io/openliberty/guides/inventory/InventoryResource.java[tags=**;!copyright]
----

=== Building and running the application

The starting Java project, which you can find in the `start` directory, is a multi-module Maven
project. It is made up of the `system` and `inventory` microservices. Each microservice resides in its own directory,
`start/system` and `start/inventory`. Both of these directories contain a Dockerfile, which is necessary
for building the Docker images. If you're unfamiliar with Dockerfiles, check out the
https://openliberty.io/guides/containerize.html[Containerizing microservices^] guide.

The `mvn package` command builds the application and then packages it into a Docker image. 
To build the Docker image, it uses the `dockerfile-maven` plug-in.

****
[system]#*{win}*#

On the Docker Desktop General Setting page, ensure that the option `Expose daemon on 
tcp://localhost:2375 without TLS` is enabled. This configuration is required by the `dockerfile-maven` 
part of the build.

****

Navigate to the `start` directory and run the following command:

[role=command]
```
mvn package
```

To verify that the images for your `system` and `inventory` microservices are built, run the `docker images` command to list all local Docker images.

[role=command]
```
docker images
```

Your two images `system` and `inventory` should appear in the list of all Docker images:

[role="no_copy"]
----
REPOSITORY    TAG         
inventory     1.0-SNAPSHOT
system        1.0-SNAPSHOT
----

To deploy your microservices to the Kubernetes cluster, use the following command:

[role=command]
```
kubectl apply -f kubernetes.yaml
```

You will see an output similar to the following:

[role="no_copy"]
----
service/system-service created
service/inventory-service created
deployment.apps/system-deployment created
deployment.apps/inventory-deployment created
gateway.networking.istio.io/sys-app-gateway created
----

View the `traffic.yaml` file. It contains two virtual services. A virtual service defines how requests are routed to your applications.

Deploy the resources defined in the `traffic.yaml` file

[role=command]
```
kubectl apply -f traffic.yaml
```

Check that all of the deployments are available:

[role=command]
```
kubectl get deployments
```

Wait until all of your deployments are ready and available before making requests to your microservices.

[source, role="no_copy"]
----
NAME                     READY     UP-TO-DATE   AVAILABLE   AGE
inventory-deployment     1         1            1           1m
system-deployment        1         1            1           1m
----

You will make a request to the `system` service from the `inventory` service to access the 
JVM system properties of your running container. The Istio [hotspot=gateway file=0]`gateway` is the entry point for HTTP requests to the cluster. 
The `Host` header of your `system` service and `inventory` service to be `system.example.com` and `inventory.example.com`, respectively. 
You can set the `Host` header with the `-H` option of the `curl` command.

Make a request to the service by using `curl`:

****
[system]#*{win} | {mac}*#


[role=command]
```
curl -H Host:inventory.example.com http://localhost/inventory/systems/system-service -I
```

[system]#*{linux}*#


[role=command]
```
curl -H Host:inventory.example.com http://`minikube ip`:31380/inventory/systems/system-service -I
```
****

You will see the following output:

[source, role="no_copy"]
----
HTTP/1.1 500 Internal Server Error
content-length: 18
content-type: text/plain
date: Thu, 15 Aug 2019 13:21:57 GMT
server: istio-envoy
----

The request returns a `500` response code, as the `system` service is unavailable.
This is because the request timed out and the `@Timeout` annotation returned a `TimeoutException`.
You will implement a Retry policy to retry the request to the `system` service. 

kubernetes.yaml
[source, yaml, linenums, role='code_column']
----
include::finish/kubernetes.yaml[]
----


=== Adding the MicroProfile @Retry annotation

A request to a service might fail for many different reasons. The default Retry policy initiates a retry for every `java.lang.Exception`. 
However, you can base a Retry policy on a specific exception by using the `retryOn` parameter. You can identify more than one exception 
as an array of values. For example, `@Retry(retryOn = {RuntimeException.class, TimeoutException.class})`.

You can set limits on the number of retry attempts to prevent a busy service from becoming overloaded with retry requests.
The `@Retry` annotation has the `maxRetries` parameter to limit the number of retry attempts. The default number for `maxRetries` is 3 requests.
The integer value must be greater than or equal to -1. A value of -1 indicates to continue retrying indefinitely.

To retry the requests to your `system` service after a `TimeoutException` has occurred, add the `@Retry` annotation.

[role="code_command hotspot", subs="quotes"]
----
#Update the `InventoryResource.java` file.#
`InventoryResource.java`
----
finish/inventory/src/main/java/io/openliberty/guides/inventory/InventoryResource.java
[source, java, linenums, role='code_column']
----
include::finish/inventory/src/main/java/io/openliberty/guides/inventory/InventoryResource.java[tags=**;!copyright]
----
[role="edit_command_text"]
Add the [hotspot=mpRetry file=0]`@Retry` annotation before the [hotspot=getPropertiesForHost file=0]`getPropertiesForHost()` method, 
to retry the service request a maximum of 3 times, only when an `TimeoutException` occurs.

Rebuild your application to integrate the Retry policy into your microservices:

[role=command]
```
mvn package -Ddockerfile.skip=true
```

The `dockerfile.skip=true` flag skips rebuilding the Docker images, since they were not modified.

Deploy your microservices again:

[role=command]
```
kubectl apply -f kubernetes.yaml
kubectl apply -f traffic.yaml
```

Make a request to the service by using `curl`:

****
[system]#*{win} | {mac}*#


[role=command]
```
curl -H Host:inventory.example.com http://localhost/inventory/systems/system-service -I
```

[system]#*{linux}*#


[role=command]
```
curl -H Host:inventory.example.com http://`minikube ip`:31380/inventory/systems/system-service -I
```
****

You will see the following output:

[source, role="no_copy"]
----
HTTP/1.1 200 OK
x-powered-by: Servlet/4.0
content-type: application/json
date: Thu, 15 Aug 2019 13:23:18 GMT
content-language: en-US
x-envoy-upstream-service-time: 2470
server: istio-envoy
transfer-encoding: chunked
----

After the request retries, the `system` service becomes available and the request is successful with a `200` response code.

To see the number of times the service was retried:

IN PROGRESS


// =================================================================================================
// Enabling Istio fault tolerance
// =================================================================================================

== Enabling Istio fault tolerance

As mentioned before, you will add Istio fault handling to your microservices.

You previously implemented the Retry policy to retry requests to your `system` service using MicroProfile Fault Tolerance.
This Retry policy can also be implemented with Istio fault tolerance.

[role="code_command hotspot", subs="quotes"]
----
#Update the `traffic.yaml` file.#
`traffic.yaml`
----
finish/traffic.yaml
[source, yaml, linenums, role='code_column']
----
include::finish/traffic.yaml[tags=**;!copyright]
----
[role="edit_command_text"]
Add the [hotspot=istioRetry file=0]`retries` field in the [hotspot]`traffic.yaml` file, 



=== Conflicts with MP Retry and Istio Retry


=== Turning off MicroProfile Fault Tolerance

Add config to kubernetes.yaml:

Disable mp:

[role=command]
```
kubectl apply -f config.yaml
```



=== Adding the MicroProfile @Fallback annotation



// =================================================================================================
// Testing the microservices
// =================================================================================================

== Testing the microservices

[role="code_command hotspot", subs="quotes"]
----
#Create the `SystemEndpointTest` class.#
`system/src/test/java/it/io/openliberty/guides/system/SystemEndpointTest.java`
----
SystemEndpointTest.java
[source, Java, linenums, role='code_column']
----
include::finish/system/src/test/java/it/io/openliberty/guides/system/SystemEndpointTest.java[tags=**;!copyright]
----

****
[system]#*{win} | {mac}*#

Run the command to start the tests:

[role=command]
```
mvn verify -Ddockerfile.skip=true
```

[system]#*{linux}*#

Run the command to start the tests:

[role=command]
```
mvn verify -Ddockerfile.skip=true -Dcluster.ip=`minikube ip` -Dport=31380
```

The `cluster.ip` and `port` parameters refer to the IP address and port for the {istio} gateway.
****

The `dockerfile.skip=true` flag skips rebuilding the {docker} images.

If the tests pass, then you should see output similar to the following example:

[source, role="no_copy"]
----
-------------------------------------------------------
 T E S T S
-------------------------------------------------------
Running it.io.openliberty.guides.system.SystemEndpointTest
Tests run: 3, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 0.503 s - in it.io.openliberty.guides.system.SystemEndpointTest

Results:

Tests run: 3, Failures: 0, Errors: 0, Skipped: 0
----



// =================================================================================================
// Tearing down your environment
// =================================================================================================

== Tearing down your environment

You might want to teardown all the deployed resources as a cleanup step.

Delete your resources from the cluster.

[role=command]
```
kubectl delete -f kubernetes.yaml
kubectl delete -f traffic.yaml
kubectl delete -f config.yaml
```

Delete the `istio-injection` label from the default namespace. The hyphen immediately
after the label name indicates that the label should be deleted.

[role=command]
```
kubectl label namespace default istio-injection-
```

Navigate to the directory where you extracted {istio} and delete the {istio} resources from the cluster.

[role=command]
```
kubectl delete -f install/kubernetes/istio-demo.yaml
```

****
[system]#*{linux} | {mac}*#

[role=command]
```
for i in install/kubernetes/helm/istio-init/files/crd*yaml; do kubectl delete -f $i; done
```

[system]#*{win}*#
[role=command]
```
FOR %i in (install\kubernetes\helm\istio-init\files\crd*yaml) DO kubectl delete -f %i
```
****


[role=command]
include::{common-includes}/kube-minikube-teardown.adoc[]

// =================================================================================================
// finish
// =================================================================================================

== Great work! You're done!

You learned how to use the Retry policy to make your microservice more resilient to failures 
and how to build a fallback mechanism for your microservices.

// Include the below from the guides-common repo to tell users how they can contribute to the guide

include::{common-includes}/attribution.adoc[subs="attributes"]
