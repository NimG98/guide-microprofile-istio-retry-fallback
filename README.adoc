// INSTRUCTION: Please remove all comments that start INSTRUCTION prior to commit. Most comments should be removed, although not the copyright.
// INSTRUCTION: The copyright statement must appear at the top of the file
//
// Copyright (c) 2018, 2019 IBM Corporation and others.
// Licensed under Creative Commons Attribution-NoDerivatives
// 4.0 International (CC BY-ND 4.0)
//   https://creativecommons.org/licenses/by-nd/4.0/
//
// Contributors:
//     IBM Corporation
//
:projectid: microprofile-istio-retry
:page-layout: guide-multipane
:page-duration: 30 minutes
:page-releasedate: 2019-09-10
:page-description: Explore how to use Fault Tolerance with MicroProfile and Istio
:page-tags: ['Kubernetes']
:page-permalink: /guides/{projectid}
:page-related-guides: ['istio-intro', 'microprofile-fallback', `iguide-retry-timeout`]
:common-includes: https://raw.githubusercontent.com/OpenLiberty/guides-common/master
:source-highlighter: prettify
:page-seo-title: Retry and Fallback with MicroProfile and Istio Fault Tolerance
:page-seo-description: How to retry services and add fallback behaviour with MicroProfile and Istio Fault Tolerance
:guide-author: Open Liberty
= Building fault-tolerant microservices using Istio Retry with MicroProfile Fallback

[.hidden]
NOTE: This repository contains the guide documentation source. To view the guide in published form, view it on the https://openliberty.io/guides/{projectid}.html[Open Liberty website].


Explore how to manage the impact of failures using MicroProfile and Istio Fault Tolerance by adding retry and fallback behavior to microservice dependencies.

:kube: Kubernetes
:istio: Istio
:win: WINDOWS
:mac: MAC
:linux: LINUX
:docker: Docker
:minikube: Minikube
:maven: Maven


// =================================================================================================
// Introduction
// =================================================================================================

== What you'll learn


You will learn how to combine https://microprofile.io/[MicroProfile^] Retry and Fallback policies with https://istio.io/[Istio^] Retry 
to make your microservices more resilient to common failures like network problems.

Microservices that are created using MicroProfile can be freely deployed in a service mesh architecture to reduce the complexity 
associated with microservice functionality. Istio is a service mesh, meaning that itâ€™s a platform for managing how microservices interact with each other and the outside world.
{istio} consists of a control plane and sidecars that are injected into application pods. The sidecars contain
the https://www.envoyproxy.io/[Envoy^] proxy. You can think of Envoy as a sidecar that intercepts
and controls all the HTTP and TCP traffic to and from your container.
If you would like to learn more about Istio, check out the https://openliberty.io/guides/istio-intro.html[Managing microservice traffic using Istio^] guide.

MicroProfile and Istio both provide simple and flexible solutions to build fault-tolerant microservices. 
Fault tolerance leverages different strategies to guide the execution and result of logic.
A few fault tolerance policies that MicroProfile can offer include Retry, Timeout, Circuit Breaker, Bulkhead, and Fallback.
There is some overlap that exists between MicroProfile and Istio Fault Tolerance, such as the Retry policy.
However, Istio does not offer any fallback capabilities.
To view the available fault tolerance policies in MicroProfile and Istio, refer to the 
https://www.eclipse.org/community/eclipse_newsletter/2018/september/MicroProfile_istio.php#faulttolerance[comparison between MicroProfile and Istio fault handling^].

Use the Retry policy to fail quickly and recover from brief intermittent issues. 
An application might experience these transient failures when a microservice is undeployed, a database is overloaded by queries, 
the network connection becomes unstable, or the site host has a brief downtime. In these cases, rather than failing quickly on these transient failures, 
the Retry policy provides another chance for the request to succeed. 
Simply retrying the request might be all you need to do to make it succeed.

Fallback offers an alternative result when an execution does not complete successfully.
You will use the `@Fallback` annotations from the MicroProfile Fault Tolerance specification to define criteria 
for when to provide an alternative solution for a failed execution.

You will create a microservice ecosystem demonstrating MicroProfile Fault Tolerance with Istio fault handling.
Both libraries can be enabled when you want your microservices to have a service mesh architecture with Istio, 
and use MicroProfile to provide the extra fault tolerance policies that do not exist within Istio.
However, microservices that have the same fault tolerance policy enabled by both MicroProfile and Istio will
have a behaviour that is not as expected. 

The application that you will be working with is an `inventory` service, which collects, stores, and returns the system properties. 
It uses the `system` service to retrieve the system properties for a particular host. 
You will add fault tolerance to the `inventory` service so that it reacts accordingly when the `system` service is unavailable.


// =================================================================================================
// Prerequisites
// =================================================================================================

include::{common-includes}/kube-prereq.adoc[]

// =================================================================================================
// Getting Started
// =================================================================================================

[role='command']
include::{common-includes}/gitclone.adoc[]

// no "try what you'll build" section in this guide since it would be too long due to all setup the user will have to do.

// =================================================================================================
// Preparing your cluster and deploying Istio
// =================================================================================================

== Preparing your cluster and deploying Istio

:minikube-start: minikube start --memory=8192 --cpus=4 --kubernetes-version=v1.13.0
:docker-desktop-description: Check your settings to ensure that you have an adequate amount of memory allocated to your Docker Desktop enviornment, 8GB is recommended but 4GB should be adequate if you don't have enough RAM.
:minikube-description: The memory flag allocates 8GB of memory to your Minikube cluster. If you don't have enough RAM, then 4GB should be adequate.
[role=command]
include::{common-includes}/kube-start.adoc[leveloffset=+1]

=== Deploying Istio

First, go to the https://github.com/istio/istio/releases/latest[{istio} release page^] and download the latest stable release. Extract the archive and navigate to the directory with the extracted files.

Next, deploy the {istio} custom resource definitions. Custom resource definitions allow {istio} to define custom {kube} resources that you can use in your resource definition files.

****
[system]#*{linux} | {mac}*#

[role=command]
```
for i in install/kubernetes/helm/istio-init/files/crd*yaml; do kubectl apply -f $i; done
```

[system]#*{win}*#
[role=command]
```
FOR %i in (install\kubernetes\helm\istio-init\files\crd*yaml) DO kubectl apply -f %i
```
****

Next, deploy {istio} resources to your cluster by running the `kubectl apply` command, which creates or updates
{kube} resources defined in a yaml file. This command deploys {istio}.

[role=command]
```
kubectl apply -f install/kubernetes/istio-demo.yaml
```

Verify that {istio} was successfully deployed. All the values in the `AVAILABLE` column will have a value of `1` after
the deployment is complete.

[role=command]
```
kubectl get deployments -n istio-system
```
 
Ensure that the {istio} deployments are all available before you continue. The deployments might take a few minutes to become available. If the deployments aren't available after a few minutes, then increase the amount of memory available to your {kube} cluster. On Docker Desktop, you can increase the memory from your {docker} preferences. On {minikube}, you can increase the memory using the `--memory` flag.
[source, role="no_copy"]
----
NAME                     DESIRED   CURRENT   UP-TO-DATE   AVAILABLE   AGE
grafana                  1         1         1            1           44s
istio-citadel            1         1         1            1           44s
istio-egressgateway      1         1         1            1           44s
istio-galley             1         1         1            1           44s
istio-ingressgateway     1         1         1            1           44s
istio-pilot              1         1         1            1           44s
istio-policy             1         1         1            1           44s
istio-sidecar-injector   1         1         1            1           44s
istio-telemetry          1         1         1            1           44s
istio-tracing            1         1         1            1           43s
prometheus               1         1         1            1           44s
servicegraph             1         1         1            1           44s
----

Finally, create the `istio-injection` label and set its value to `enabled`.

[role=command]
```
kubectl label namespace default istio-injection=enabled
```

Adding this label enables automatic {istio} sidecar injection. Automatic injection means that sidecars will
automatically be injected into your pods when you deploy your application. You don't need to perform
any additional steps for the sidecars to be injected.

// =================================================================================================
// Enabling MicroProfile fault tolerance
// =================================================================================================

== Enabling MicroProfile fault tolerance

Navigate to the `start` directory to begin.

The MicroProfile Fault Tolerance API was added as a dependency to your `inventory` service's [hotspot file=0]`pom.xml` file. Look for
the dependency with the [hotspot=mpFaultTolerance file=0]`mpFaultTolerance` artifact ID. Adding this dependency allows you to use the
fault tolerance policies in your microservices.

You can also find the [hotspot=mpFaultTolerance2 file=1]`mpFaultTolerance` feature in your inventory [hotspot file=1]`server.xml` server configuration,
which turns on MicroProfile Fault Tolerance capabilities in Open Liberty.

Next, the [hotspot file=2]`SystemClient.java` file
makes a request to the `system` service through the MicroProfile Rest Client API.
If you want to learn more about MicroProfile Rest Client,
you can follow the https://openliberty.io/guides/microprofile-rest-client.html[Consuming RESTful services with template interfaces^] guide.
To simulate that your `system` service is temporarily down, due to brief intermittent issues, you will pause the pod that is associated
with your `system` service, and try to send requests to the service. When the `system` pod is paused, requests to the service return a `500` status code.
The [hotspot=getPropertiesHelper file=2]`getPropertiesHelper()` in [hotspot file=2]`SystemClient.java` checks for status codes that are not OK, 
i.e. the status code is not `200`, and throws an [hotspot=throwIO file=2]`IOException`. 


pom.xml
[source, xml, linenums, role='code_column']
----
include::finish/inventory/pom.xml[]
----

server.xml
[source, xml, linenums, role='code_column']
----
include::finish/inventory/src/main/liberty/config/server.xml[]
----

SystemClient.java
[source, java, linenums, role='code_column']
----
include::finish/inventory/src/main/java/io/openliberty/guides/inventory/client/SystemClient.java[tags=**;!copyright]
----

=== Building and running the application

The starting Java project, which you can find in the `start` directory, is a multi-module Maven
project. It is made up of the `system` and `inventory` microservices. Each microservice resides in its own directory,
`start/system` and `start/inventory`. Both of these directories contain a Dockerfile, which is necessary
for building the Docker images. If you're unfamiliar with Dockerfiles, check out the
https://openliberty.io/guides/containerize.html[Containerizing microservices^] guide.

The `mvn package` command builds the application and then packages it into a Docker image. 
To build the Docker image, it uses the `dockerfile-maven` plug-in.

****
[system]#*{win}*#

On the Docker Desktop General Setting page, ensure that the option `Expose daemon on 
tcp://localhost:2375 without TLS` is enabled. This configuration is required by the `dockerfile-maven` 
part of the build.

****

Navigate to the `start` directory and run the following command:

[role=command]
```
mvn package
```

To verify that the images for your `system` and `inventory` microservices are built, run the `docker images` command to list all local Docker images.

[role=command]
```
docker images
```

Your two images `system` and `inventory` should appear in the list of all Docker images:

[role="no_copy"]
----
REPOSITORY    TAG         
inventory     1.0-SNAPSHOT
system        1.0-SNAPSHOT
----

To deploy your microservices to the Kubernetes cluster, use the following command:

[role=command]
```
kubectl apply -f istio.yaml
```

You will see an output similar to the following:

[role="no_copy"]
----
service/system-service created
service/inventory-service created
deployment.apps/system-deployment created
deployment.apps/inventory-deployment created
gateway.networking.istio.io/sys-app-gateway created
----

View the `traffic.yaml` file. It contains two virtual services. A virtual service defines how requests are routed to your applications.

Deploy the resources defined in the `traffic.yaml` file

[role=command]
```
kubectl apply -f traffic.yaml
```

Run the following command to check the status of your pods:

[role=command]
```
kubectl get pods
```

If all the pods are healthy and running, you will see an output similar to the following:

[source, role="no_copy"]
----
NAME                                    READY     STATUS    RESTARTS   AGE
inventory-deployment-645767664f-nbtd9   1/1       Running   0          30s
system-deployment-6bd97d9bf6-4ccds      1/1       Running   0          30s
----

Check that all of the deployments are available. You need to wait until all of your deployments are 
ready and available before making requests to your microservices.

[role=command]
```
kubectl get deployments
```

[source, role="no_copy"]
----
NAME                     READY     UP-TO-DATE   AVAILABLE   AGE
inventory-deployment     1         1            1           1m
system-deployment        1         1            1           1m
----

Pause the `system` service pod to simulate that the service is unavailable. Replace `[system-pod-name]` with the pod name associated with
your `system` service, which you previously saw when running the `kubectl get pods` command.

[role=command]
```
kubectl exec -it [system-pod-name] /opt/ol/wlp/bin/server pause
```

You will make a request to the `system` service from the `inventory` service to access the 
JVM system properties of your running container. The Istio [hotspot=gateway file=0]`gateway` is the entry point for HTTP requests to the cluster. 
The `Host` header of your `system` service and `inventory` service to be `system.example.com` and `inventory.example.com`, respectively. 
You can set the `Host` header with the `-H` option of the `curl` command.

Make a request to the service by using `curl`:

****
[system]#*{win} | {mac}*#


[role=command]
```
curl -H Host:inventory.example.com http://localhost/inventory/systems/system-service -I
```

[system]#*{linux}*#


[role=command]
```
curl -H Host:inventory.example.com http://`minikube ip`:31380/inventory/systems/system-service -I
```
****

You will see the following output:

[source, role="no_copy"]
----
HTTP/1.1 500 Internal Server Error
content-length: 18
content-type: text/plain
date: Thu, 15 Aug 2019 13:21:57 GMT
server: istio-envoy
----

The request returns a `500` response code, as the `system` service is unavailable.
You will implement a Retry policy to retry the requests to the `system` service. 

istio.yaml
[source, yaml, linenums, role='code_column']
----
include::finish/istio.yaml[]
----


=== Adding the MicroProfile @Retry annotation

A request to a service might fail for many different reasons. The default Retry policy initiates a retry for every `java.lang.Exception`. 
However, you can base a Retry policy on a specific exception by using the `retryOn` parameter. You can identify more than one exception 
as an array of values. For example, `@Retry(retryOn = {RuntimeException.class, TimeoutException.class})`.

You can set limits on the number of retry attempts to prevent a busy service from becoming overloaded with retry requests.
The `@Retry` annotation has the `maxRetries` parameter to limit the number of retry attempts. The default number for `maxRetries` is 3 requests.
The integer value must be greater than or equal to -1. A value of -1 indicates to continue retrying indefinitely.

To retry the requests to your `system` service after an `IOException` has occurred, add the `@Retry` annotation.

[role="code_command hotspot", subs="quotes"]
----
#Update the `InventoryResource.java` file.#
`InventoryResource.java`
----
finish/inventory/src/main/java/io/openliberty/guides/inventory/InventoryResource.java
[source, java, linenums, role='code_column']
----
include::finish/inventory/src/main/java/io/openliberty/guides/inventory/InventoryResource.java[tags=**;!copyright]
----
[role="edit_command_text"]
Add the [hotspot=mpRetry file=0]`@Retry` annotation before the [hotspot=getPropertiesForHost file=0]`getPropertiesForHost()` method, 
to retry the service request a maximum of 3 times, only when an `IOException` occurs.

If the resources in your cluster are still running from before, delete them before proceeding:

[role=command]
```
kubectl delete -f istio.yaml
kubectl delete -f traffic.yaml
```

Rebuild your application to integrate the Retry policy into your microservices:

[role=command]
```
mvn package
```

Deploy your microservices again, and wait until all of your deployments are ready and available:

[role=command]
```
kubectl apply -f istio.yaml
kubectl apply -f traffic.yaml
```

Pause the `system` service pod to simulate that the service is unavailable. Replace `[system-pod-name]` with the pod name associated with
your `system` service.

[role=command]
```
kubectl exec -it [system-pod-name] /opt/ol/wlp/bin/server pause
```

Make a request to the service by using `curl`:

****
[system]#*{win} | {mac}*#


[role=command]
```
curl -H Host:inventory.example.com http://localhost/inventory/systems/system-service -I
```

[system]#*{linux}*#


[role=command]
```
curl -H Host:inventory.example.com http://`minikube ip`:31380/inventory/systems/system-service -I
```
****

You will see that the request still returns a `500` response code, as the `system` service is unavailable.
This time however, the request was retried several times before failing.

To see the number of times that the service was retried, check the logs of the `system` pod using the `kubectl logs` command.
Remember to replace `[system-pod-name]` with the pod name associated with your `system` service.

****
[system]#*{mac} | {linux}*#


[role=command]
```
kubectl logs [system-pod-name] -c istio-proxy | grep -c system-service:9080
```

[system]#*{win}*#


[role=command]
```
kubectl logs [system-pod-name] -c istio-proxy | find /C "system-service:9080"
```
****

The above command returns 4, since there were a total of 4 requests made to the `system` service,
including the initial request and the 3 retries that were made after the initial request failed. 


// =================================================================================================
// Enabling Istio fault tolerance
// =================================================================================================

== Enabling Istio fault tolerance

You previously implemented the Retry policy to retry requests to your `system` service using MicroProfile Fault Tolerance.
This Retry policy can also be implemented with Istio fault tolerance.

If the resources in your cluster are still running from before, delete them before proceeding:

[role=command]
```
kubectl delete -f istio.yaml
kubectl delete -f traffic.yaml
```

[role="code_command hotspot", subs="quotes"]
----
#Update the `traffic.yaml` file.#
`traffic.yaml`
----
finish/traffic.yaml
[source, yaml, linenums, role='code_column']
----
include::finish/traffic.yaml[]
----
[role="edit_command_text"]
Add the [hotspot=istioRetry file=0]`retries` field in the [hotspot file=0]`traffic.yaml` file, to allow Istio to
retry requests to the `system` service a maximum of 2 times, when the request returns any `5xx` response code. 

The [hotspot=attempts file=0]`attempts` field is required in the configuration of Istio's Retry policy, and it indicates the
maximum number of retries that will be attempted for a given request. To retry a request on specific conditions, the
[hotspot=retryOn file=0]`retryOn` field is used. Since your paused `system` service responds with a `500` response code, 
you set [hotspot=retryOn file=0]`retryOn` to be `5xx`. 
Other https://www.envoyproxy.io/docs/envoy/latest/configuration/http_filters/router_filter#x-envoy-retry-on[retry conditions^] 
can also be specified in [hotspot=retryOn file=0]`retryOn`. An optional field that can be provided to Istio's Retry policy
is `perTryTimeout`, which indicates the amount of time that is allocated to each retry attempt. 

After integrating an Istio Retry policy into your microservices, deploy your microservices again, 
and wait until all of your deployments are ready and available:

[role=command]
```
kubectl apply -f istio.yaml
kubectl apply -f traffic.yaml
```

Pause the `system` service pod to simulate that the service is unavailable. Replace `[system-pod-name]` with the pod name associated with
your `system` service.

[role=command]
```
kubectl exec -it [system-pod-name] /opt/ol/wlp/bin/server pause
```

Make a request to the service by using `curl`:

****
[system]#*{win} | {mac}*#


[role=command]
```
curl -H Host:inventory.example.com http://localhost/inventory/systems/system-service -I
```

[system]#*{linux}*#


[role=command]
```
curl -H Host:inventory.example.com http://`minikube ip`:31380/inventory/systems/system-service -I
```
****

You will see that the request still returns a `500` response code, as the `system` service is unavailable.
This time however, the request was retried several times with Istio before failing.

To see the number of times that the service was retried, check the logs of the `system` pod using the `kubectl logs` command.
Remember to replace `[system-pod-name]` with the pod name associated with your `system` service.

****
[system]#*{mac} | {linux}*#


[role=command]
```
kubectl logs [system-pod-name] -c istio-proxy | grep -c system-service:9080
```

[system]#*{win}*#


[role=command]
```
kubectl logs [system-pod-name] -c istio-proxy | find /C "system-service:9080"
```
****

The above command returns 12, indicating that a total of 12 requests are made to the `system` service.
However, you enabled Istio to retry a maximum of 2 times, and hence the total number of requests to the `system` service should be 3.
This is because the number of retries set by MicroProfile conflicts with the number of retries set by Istio.
To prevent this behaviour, you will disable some of MicroProfile's capabilities, including Retry, to have your `system` service 
retry the exact number of times indicated in Istio's Retry policy. 


// =================================================================================================
// Turning off MicroProfile Fault Tolerance except Fallback
// =================================================================================================

== Turning off MicroProfile Fault Tolerance except Fallback

=== Conflict with MicroProfile Retry and Istio Retry

When microservices have the same fault tolerance policy enabled by both MicroProfile and Istio, the behaviour of the policy is not as expected. 
As you have seen before, if both MicroProfile and Istio set their own Retry policies on a service,
the maximum number of retries that occur is not equivalent to either of the number of retries specified in MicroProfile or Istio. 
The microservice actually multiplies the number of MicroProfile and Istio retries. For instance, you set MicroProfile to perform 3 retries,
and Istio to perform 2 retries, however you noticed using `kubectl logs` that a total of 12 requests were made to the `system` service.
Individually, MicroProfile would have sent a total of 4 requests, including the initial request and 3 retries. Without MicroProfile,
Istio would have sent a total of 3 requests, including the initial request and 2 retries. When both retry policies are enabled,
the 4 requests by MicroProfile are multiplied by the 3 requests by Istio, causing a total of 12 requests.

In the case that you want your microservices to use a service-mesh architecture, such as Istio, and use it's fault handling without interfering with
the service's MicroProfile properties, you can use a config property offered by MicroProfile.
      
=== Setting the MP_Fault_Tolerance_NonFallback_Enabled config property

MicroProfile Fault Tolerance offers a config property `MP_Fault_Tolerance_NonFallback_Enabled` that disables all 
MicroProfile Fault Tolerance capabilities except fallback. If `MP_Fault_Tolerance_NonFallback_Enabled` is set to false, all of the MicroProfile
annotations, including `@Retry`, will switch off and only keep `@Fallback` available to use.

You will use the `MP_Fault_Tolerance_NonFallback_Enabled` config property to disable the retries performed by MicroProfile, 
to only allow Istio to perform the retries.

[role="code_command hotspot file=0", subs="quotes"]
----
#Create the `config.yaml`.#
`config.yaml`
----

config.yaml
[source, Text, linenums, indent=0, role="code_column"]
----
include::finish/config.yaml[]
----

In the [hotspot file=0]`config.yaml`, the [hotspot=nonFallback file=0]`MP_Fault_Tolerance_NonFallback_Enabled` config property is set to false in a [hotspot=configMap file=0]`ConfigMap`. 
ConfigMaps store configuration settings about a Kubernetes pod. This configuration needs to be loaded into the pod as an environment variable to be 
used by the pod's containers. The environment variables are defined in the pod's specification using the `envFrom` field. 

[role="code_command hotspot", subs="quotes"]
----
#Update the `istio.yaml` file.#
`istio.yaml`
----
finish/istio.yaml
[source, yaml, linenums, role='code_column']
----
include::finish/istio.yaml[]
----
[role="edit_command_text"]
To inject the [hotspot=configMap file=0]`ConfigMap` with the [hotspot=nonFallback file=0]`MP_Fault_Tolerance_NonFallback_Enabled` property into your pods, 
add the [hotspot=invConfig file=1]`envFrom` field in the [hotspot file=1]`istio.yaml` file. 

The [hotspot=configName file=0]`name` of the [hotspot=configMap file=0]`ConfigMap`, which is [hotspot=configName file=0]`inventory-config`, 
becomes the environment variable [hotspot=configName2 file=1]`name` that is specified in the [hotspot=invConfig file=1]`envFrom` field. 

If the resources in your cluster are still running from before, delete them before proceeding:

[role=command]
```
kubectl delete -f istio.yaml
kubectl delete -f traffic.yaml
```

Deploy your microservices again, and wait until all of your deployments are ready and available:

[role=command]
```
kubectl apply -f istio.yaml
kubectl apply -f traffic.yaml
```

Turn off all MicroProfile capabilities, except fallback, by creating the [hotspot=configMap file=0]`ConfigMap` in [hotspot file=0]`config.yaml`:

[role=command]
```
kubectl apply -f config.yaml
```

Pause the `system` service pod to simulate that the service is unavailable. Replace `[system-pod-name]` with the pod name associated with
your `system` service.

[role=command]
```
kubectl exec -it [system-pod-name] /opt/ol/wlp/bin/server pause
```

Make a request to the service by using `curl`:

****
[system]#*{win} | {mac}*#


[role=command]
```
curl -H Host:inventory.example.com http://localhost/inventory/systems/system-service -I
```

[system]#*{linux}*#


[role=command]
```
curl -H Host:inventory.example.com http://`minikube ip`:31380/inventory/systems/system-service -I
```
****

You will see that the request still returns a `500` response code, as the `system` service is unavailable.
This time however, the request was retried several times with Istio, without any retries from MicroProfile.

To see the number of times that the service was retried, check the logs of the `system` pod using the `kubectl logs` command.
Remember to replace `[system-pod-name]` with the pod name associated with your `system` service.

****
[system]#*{mac} | {linux}*#


[role=command]
```
kubectl logs [system-pod-name] -c istio-proxy | grep -c system-service:9080
```

[system]#*{win}*#


[role=command]
```
kubectl logs [system-pod-name] -c istio-proxy | find /C "system-service:9080"
```
****

The above command returns 3, indicating that a total of 3 requests are made to the `system` service.
Since MicroProfile's Retry policy is disabled, only Istio's retries are performed.
You enabled Istio to retry a maximum of 2 times, and hence the total number of requests to the `system` service is 3.


=== Adding the MicroProfile @Fallback annotation

Since retrying the requests to your `system` service is still not successful, you need a plan to "fall back" on when the requests fail. 
You will create a fallback method to deal with the failed requests to your `system` service.

Although you disabled MicroProfile `@Retry` and other MicroProfile Fault Tolerance policies using the 
[hotspot=nonFallback file=0]`MP_Fault_Tolerance_NonFallback_Enabled` config property, the fallback policy is still available to use.
As mentioned before, Istio does not offer any fallback capabilities, and so the MicroProfile `@Fallback` annotation must be used 
to add fallback to your microservices.

The `@Fallback` annotation dictates a method to call when the original method encounters a failed execution. 
If your microservices have a Retry policy specified, then the fallback occurs after all of the retries have failed. 

[role="code_command hotspot", subs="quotes"]
----
#Update the `InventoryResource.java` file.#
`InventoryResource.java`
----
finish/inventory/src/main/java/io/openliberty/guides/inventory/InventoryResource.java
[source, java, linenums, role='code_column']
----
include::finish/inventory/src/main/java/io/openliberty/guides/inventory/InventoryResource.java[tags=**;!copyright]
----
[role="edit_command_text"]
Create the [hotspot=fallbackMethod file=0]`getPropertiesFallback()` method. 
Add the [hotspot=fallback file=0]`@Fallback` annotation before the [hotspot=getPropertiesForHost file=0]`getPropertiesForHost()` method, 
to call the [hotspot=fallbackMethod file=0]`getPropertiesFallback()` method when a failure occurs.


The [hotspot=fallbackMethod file=0]`getPropertiesFallback()` method, which is the designated fallback method for the original 
[hotspot=getPropertiesForHost file=0]`getPropertiesForHost()` method, prints out a warning message in the browser saying that 
the `system` service may not be running.

If the resources in your cluster are still running from before, delete them before proceeding:

[role=command]
```
kubectl delete -f istio.yaml
kubectl delete -f traffic.yaml
```

Deploy your microservices again, and wait until all of your deployments are ready and available:

[role=command]
```
kubectl apply -f istio.yaml
kubectl apply -f traffic.yaml
```

Turn off all MicroProfile capabilities, except fallback, by creating the [hotspot=configMap file=0]`ConfigMap` in [hotspot file=0]`config.yaml`:

[role=command]
```
kubectl apply -f config.yaml
```

Pause the `system` service pod to simulate that the service is unavailable. Replace `[system-pod-name]` with the pod name associated with
your `system` service.

[role=command]
```
kubectl exec -it [system-pod-name] /opt/ol/wlp/bin/server pause
```

Make a request to the service by using `curl`:

****
[system]#*{win} | {mac}*#


[role=command]
```
curl -H Host:inventory.example.com http://localhost/inventory/systems/system-service -I
```

[system]#*{linux}*#


[role=command]
```
curl -H Host:inventory.example.com http://`minikube ip`:31380/inventory/systems/system-service -I
```
****

You will see that the request is now successful and returns a `200` response code, since the fallback method is called 
when the `system` service is not available.


To see the number of times that the service is retried before the fallback method is called, check the logs of the `system` pod using the `kubectl logs` command.
Remember to replace `[system-pod-name]` with the pod name associated with your `system` service.

****
[system]#*{mac} | {linux}*#


[role=command]
```
kubectl logs [system-pod-name] -c istio-proxy | grep -c system-service:9080
```

[system]#*{win}*#


[role=command]
```
kubectl logs [system-pod-name] -c istio-proxy | find /C "system-service:9080"
```
****

The above command returns 3, indicating that a total of 3 requests are made to the `system` service.
You enabled Istio to retry a maximum of 2 times, and hence the total number of requests to the `system` service is 3.
However, since all of these requests failed, the fallback method [hotspot=fallbackMethod file=0]`getPropertiesFallback()` is called. 

// =================================================================================================
// Testing the microservices
// =================================================================================================

== Testing the microservices

Run the command to start the tests:

****
[system]#*{win} | {mac}*#

[role=command]
```
mvn verify -Ddockerfile.skip=true
```

[system]#*{linux}*#

[role=command]
```
mvn verify -Ddockerfile.skip=true -Dcluster.ip=`minikube ip` -Dport=31380
```

The `cluster.ip` and `port` parameters refer to the IP address and port for the {istio} gateway.
****

The `dockerfile.skip=true` flag skips rebuilding the {docker} images.

If the tests pass, then you should see output similar to the following example:

[source, role="no_copy"]
----
-------------------------------------------------------
 T E S T S
-------------------------------------------------------
Running it.io.openliberty.guides.system.SystemEndpointTest
Tests run: 3, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 0.503 s - in it.io.openliberty.guides.system.SystemEndpointTest

Results:

Tests run: 3, Failures: 0, Errors: 0, Skipped: 0
----


// =================================================================================================
// Tearing down your environment
// =================================================================================================

== Tearing down your environment

You might want to teardown all the deployed resources as a cleanup step.

Delete your resources from the cluster.

[role=command]
```
kubectl delete -f istio.yaml
kubectl delete -f traffic.yaml
kubectl delete -f config.yaml
```

Delete the `istio-injection` label from the default namespace. The hyphen immediately
after the label name indicates that the label should be deleted.

[role=command]
```
kubectl label namespace default istio-injection-
```

Navigate to the directory where you extracted {istio} and delete the {istio} resources from the cluster.

[role=command]
```
kubectl delete -f install/kubernetes/istio-demo.yaml
```

****
[system]#*{linux} | {mac}*#

[role=command]
```
for i in install/kubernetes/helm/istio-init/files/crd*yaml; do kubectl delete -f $i; done
```

[system]#*{win}*#
[role=command]
```
FOR %i in (install\kubernetes\helm\istio-init\files\crd*yaml) DO kubectl delete -f %i
```
****


[role=command]
include::{common-includes}/kube-minikube-teardown.adoc[]

// =================================================================================================
// finish
// =================================================================================================

== Great work! You're done!

You learned how to use the Retry policy to make your microservice more resilient to failures 
and how to build a fallback mechanism for your microservices.

// Include the below from the guides-common repo to tell users how they can contribute to the guide

include::{common-includes}/attribution.adoc[subs="attributes"]
