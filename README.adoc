// INSTRUCTION: Please remove all comments that start INSTRUCTION prior to commit. Most comments should be removed, although not the copyright.
// INSTRUCTION: The copyright statement must appear at the top of the file
//
// Copyright (c) 2019 IBM Corporation and others.
// Licensed under Creative Commons Attribution-NoDerivatives
// 4.0 International (CC BY-ND 4.0)
//   https://creativecommons.org/licenses/by-nd/4.0/
//
// Contributors:
//     IBM Corporation
//
:projectid: microprofile-istio-retry
:page-layout: guide-multipane
:page-duration: 30 minutes
:page-releasedate: 2019-09-10
:page-description: Explore how to use MicroProfile and Istio Fault Tolerance
:page-tags: ['Kubernetes']
:page-permalink: /guides/{projectid}
:page-related-guides: ['istio-intro', 'microprofile-fallback', `iguide-retry-timeout`]
:common-includes: ../guides-common/
:source-highlighter: prettify
:page-seo-title: Retry and Fallback with MicroProfile and Istio Fault Tolerance
:page-seo-description: How to retry services and add fallback behaviour with MicroProfile and Istio Fault Tolerance
:guide-author: Open Liberty
= Running fault-tolerant microservices with Istio Retry and MicroProfile Fallback

[.hidden]
NOTE: This repository contains the guide documentation source. To view the guide in published form, view it on the https://openliberty.io/guides/{projectid}.html[Open Liberty website].


Explore how to manage the impact of failures using MicroProfile and Istio Fault Tolerance by adding retry and fallback behavior to microservices.

:kube: Kubernetes
:istio: Istio
:win: WINDOWS
:mac: MAC
:linux: LINUX
:docker: Docker
:minikube: Minikube
:maven: Maven


// =================================================================================================
// Introduction
// =================================================================================================

== What you'll learn


You will learn how to combine https://download.eclipse.org/microprofile/microprofile-fault-tolerance-2.0/microprofile-fault-tolerance-spec.html#retry[MicroProfile Retry^] 
and https://download.eclipse.org/microprofile/microprofile-fault-tolerance-2.0/microprofile-fault-tolerance-spec.html#fallback[Fallback^]
policies with https://istio.io/docs/concepts/traffic-management/#timeouts-and-retries[Istio Retry^] 
to make your microservices more resilient to common failures like network problems.

Microservices that are created using MicroProfile can be freely deployed in a service mesh architecture to reduce the complexity 
associated with microservice functionality. Istio is a service mesh, meaning that itâ€™s a platform for managing how microservices interact with each other and the outside world.
{istio} consists of a control plane and sidecars that are injected into application pods. The sidecars contain
the https://www.envoyproxy.io/[Envoy^] proxy. You can think of Envoy as a sidecar that intercepts
and controls all the HTTP and TCP traffic to and from your container.
If you would like to learn more about Istio, check out the https://openliberty.io/guides/istio-intro.html[Managing microservice traffic using Istio^] guide.

MicroProfile and Istio both provide simple and flexible solutions to build fault-tolerant microservices. 
Fault tolerance leverages different strategies to guide the execution and result of logic.
A few fault tolerance policies that MicroProfile can offer include Retry, Timeout, Circuit Breaker, Bulkhead, and Fallback.
There is some overlap that exists between MicroProfile and Istio Fault Tolerance, such as the Retry policy.
However, Istio does not offer any fallback capabilities.
To view the available fault tolerance policies in MicroProfile and Istio, refer to the 
https://www.eclipse.org/community/eclipse_newsletter/2018/september/MicroProfile_istio.php#faulttolerance[comparison between MicroProfile and Istio fault handling^].

Use the Retry policy to fail quickly and recover from brief intermittent issues. 
An application might experience these transient failures when a microservice is undeployed, a database is overloaded by queries, 
the network connection becomes unstable, or the site host has a brief downtime. In these cases, rather than failing quickly on these transient failures, 
the Retry policy provides another chance for the request to succeed. 
Simply retrying the request might be all you need to do to make it succeed.

Fallback offers an alternative result when an execution does not complete successfully.
You will use the `@Fallback` annotation from the MicroProfile Fault Tolerance specification to define criteria 
for when to provide an alternative solution for a failed execution.

You will create a microservice ecosystem demonstrating MicroProfile Fault Tolerance with Istio fault handling.
Both libraries can be enabled when you want your microservices to have a service mesh architecture with Istio, 
and use MicroProfile to provide the extra fault tolerance policies that do not exist within Istio.
However, microservices that have the same fault tolerance policy enabled by both MicroProfile and Istio will
have a behaviour that is not as expected. 

The application that you will be working with is an `inventory` service, which collects, stores, and returns the system properties. 
It uses the `system` service to retrieve the system properties for a particular host. 
You will add fault tolerance to the `inventory` service so that it reacts accordingly when the `system` service is unavailable.


// =================================================================================================
// Prerequisites
// =================================================================================================

include::{common-includes}/kube-prereq.adoc[]

// =================================================================================================
// Getting Started
// =================================================================================================

[role='command']
include::{common-includes}/gitclone.adoc[]

// no "try what you'll build" section in this guide since it would be too long due to all setup the user will have to do.

// =================================================================================================
// Preparing your cluster and deploying Istio
// =================================================================================================

:minikube-start: minikube start --memory=8192 --cpus=4 --kubernetes-version=v1.13.0
:docker-desktop-description: Check your settings to ensure that you have an adequate amount of memory allocated to your Docker Desktop enviornment, 8GB is recommended but 4GB should be adequate if you don't have enough RAM.
:minikube-description: The memory flag allocates 8GB of memory to your Minikube cluster. If you don't have enough RAM, then 4GB should be adequate.
[role=command]
include::{common-includes}/kube-start.adoc[]

== Deploying Istio

First, go to the https://github.com/istio/istio/releases/latest[{istio} release page^] and download the latest stable release. Extract the archive and navigate to the directory with the extracted files.

Next, deploy the {istio} custom resource definitions. Custom resource definitions allow {istio} to define custom {kube} resources that you can use in your resource definition files.

****
[system]#*{linux} | {mac}*#

[role=command]
```
for i in install/kubernetes/helm/istio-init/files/crd*yaml; do kubectl apply -f $i; done
```

[system]#*{win}*#
[role=command]
```
FOR %i in (install\kubernetes\helm\istio-init\files\crd*yaml) DO kubectl apply -f %i
```
****

Next, deploy {istio} resources to your cluster by running the `kubectl apply` command, which creates or updates
{kube} resources defined in a yaml file. This command deploys {istio}.

[role=command]
```
kubectl apply -f install/kubernetes/istio-demo.yaml
```

Verify that {istio} was successfully deployed. All the values in the `AVAILABLE` column will have a value of `1` after
the deployment is complete.

[role=command]
```
kubectl get deployments -n istio-system
```
 
Ensure that the {istio} deployments are all available before you continue. The deployments might take a few minutes to become available. 
If the deployments aren't available after a few minutes, then increase the amount of memory available to your {kube} cluster. 
On Docker Desktop, you can increase the memory from your {docker} preferences. On {minikube}, you can increase the memory using the `--memory` flag.

[source, role="no_copy"]
----
NAME                     DESIRED   CURRENT   UP-TO-DATE   AVAILABLE   AGE
grafana                  1         1         1            1           44s
istio-citadel            1         1         1            1           44s
istio-egressgateway      1         1         1            1           44s
istio-galley             1         1         1            1           44s
istio-ingressgateway     1         1         1            1           44s
istio-pilot              1         1         1            1           44s
istio-policy             1         1         1            1           44s
istio-sidecar-injector   1         1         1            1           44s
istio-telemetry          1         1         1            1           44s
istio-tracing            1         1         1            1           43s
prometheus               1         1         1            1           44s
servicegraph             1         1         1            1           44s
----

Finally, create the `istio-injection` label and set its value to `enabled`.

[role=command]
```
kubectl label namespace default istio-injection=enabled
```

Adding this label enables automatic {istio} sidecar injection. Automatic injection means that sidecars will
automatically be injected into your pods when you deploy your application. You don't need to perform
any additional steps for the sidecars to be injected.

// =================================================================================================
// Enabling MicroProfile fault tolerance
// =================================================================================================

== Enabling MicroProfile fault tolerance

Navigate to the `start` directory to begin.

The MicroProfile Fault Tolerance API was added as a dependency to your `inventory` service's [hotspot file=0]`pom.xml` file. Look for
the dependency with the [hotspot=mpFaultTolerance file=0]`mpFaultTolerance` artifact ID. Adding this dependency allows you to use the
fault tolerance policies in your microservices.

You can also find the [hotspot=mpFaultTolerance2 file=1]`mpFaultTolerance` feature in your inventory [hotspot file=1]`server.xml` server configuration,
which turns on MicroProfile Fault Tolerance capabilities in Open Liberty.

The [hotspot file=2]`InventoryResource.java` file
makes a request to the `system` service through the MicroProfile Rest Client API.
If you want to learn more about MicroProfile Rest Client,
you can follow the https://openliberty.io/guides/microprofile-rest-client.html[Consuming RESTful services with template interfaces^] guide.

pom.xml
[source, xml, linenums, role='code_column']
----
include::finish/inventory/pom.xml[]
----

server.xml
[source, xml, linenums, role='code_column']
----
include::finish/inventory/src/main/liberty/config/server.xml[]
----

InventoryResource.java
[source, Java, linenums, role='code_column hide_tags=copyright']
----
include::finish/inventory/src/main/java/io/openliberty/guides/inventory/InventoryResource.java[]
----

=== Adding the MicroProfile @Retry annotation

To simulate that your `system` service is temporarily down, due to brief intermittent issues, you will pause the pod that is associated
with your `system` service, and try to send requests to the service. When the `system` pod is paused, requests to the service return a `500` status code, 
and the [hotspot=getProperties file=0]`systemClient.getProperties()` in [hotspot file=0]`InventoryResource.java` throws a `ProcessingException`. 

To retry the requests to your `system` service after an `ProcessingException` has occurred, add the `@Retry` annotation.

[role="code_command hotspot", subs="quotes"]
----
#Update the `InventoryResource.java` file.#
`inventory/src/main/java/io/openliberty/guides/inventory/InventoryResource.java`
----
[role="edit_command_text"]
Add the [hotspot=mpRetry file=0]`@Retry` annotation before the [hotspot=getPropertiesForHost file=0]`getPropertiesForHost()` method, 
to retry the service request a maximum of 3 times, only when an [hotspot=exception file=1]`ProcessingException` occurs.

InventoryResource.java
[source, Java, linenums, role='code_column hide_tags=copyright']
----
include::finish/inventory/src/main/java/io/openliberty/guides/inventory/InventoryResource.java[]
----

SystemClient.java
[source, Java, linenums, role='code_column hide_tags=copyright']
----
include::finish/inventory/src/main/java/io/openliberty/guides/inventory/client/SystemClient.java[]
----

A request to a service might fail for many different reasons. The default Retry policy initiates a retry for every `java.lang.Exception`. 
However, you can base a Retry policy on a specific exception by using the `retryOn` parameter. You can identify more than one exception 
as an array of values. For example, `@Retry(retryOn = {RuntimeException.class, TimeoutException.class})`.

You can set limits on the number of retry attempts to prevent a busy service from becoming overloaded with retry requests.
The `@Retry` annotation has the `maxRetries` parameter to limit the number of retry attempts. The default number for `maxRetries` is 3 requests.
The integer value must be greater than or equal to -1. A value of -1 indicates to continue retrying indefinitely.

=== Building and running the application

Navigate to the `start` directory and run the following command to build your application and integrate the Retry policy into your microservices:

[role=command]
```
mvn package
```

Next, run the `docker build` commands to build container images for your application:

[role='command']
```
docker build -t system:1.0-SNAPSHOT system/.
docker build -t inventory:1.0-SNAPSHOT inventory/.
```

The `-t` flag in the `docker build` command allows the Docker image to be labeled (tagged) in the `name[:tag]` format. 
The tag for an image describes the specific image version. If the optional `[:tag]` tag is not specified, the `latest` tag is created by default.

To verify that the images for your `system` and `inventory` microservices are built, run the `docker images` command to list all local Docker images.

[role=command]
```
docker images
```

Your two images `system` and `inventory` should appear in the list of all Docker images:

[source, role="no_copy"]
----
REPOSITORY         TAG         
inventory          1.0-SNAPSHOT
system             1.0-SNAPSHOT
----

To deploy your microservices to the Kubernetes cluster, use the following command:

[role=command]
```
kubectl apply -f services.yaml
```

You will see an output similar to the following:

[role="no_copy"]
----
gateway.networking.istio.io/sys-app-gateway created
service/system-service created
service/inventory-service created
deployment.apps/system-deployment created
deployment.apps/inventory-deployment created
----

The `traffic.yaml` file contains two virtual services. A virtual service defines how requests are routed to your applications.

Deploy the resources defined in the `traffic.yaml` file:

[role=command]
```
kubectl apply -f traffic.yaml
```

Run the following command to check the status of your pods:

[role=command]
```
kubectl get pods
```

If all the pods are healthy and running, you will see an output similar to the following:

[source, role="no_copy"]
----
NAME                                    READY     STATUS    RESTARTS   AGE
inventory-deployment-645767664f-nbtd9   2/2       Running   0          30s
system-deployment-6bd97d9bf6-4ccds      2/2       Running   0          30s
----

Check that all of the deployments are available. You need to wait until all of your deployments are 
ready and available before making requests to your microservices.

[role=command]
```
kubectl get deployments
```

[source, role="no_copy"]
----
NAME                     READY     UP-TO-DATE   AVAILABLE   AGE
inventory-deployment     1/1       1            1           1m
system-deployment        1/1       1            1           1m
----

You will make a request to the `system` service from the `inventory` service to access the 
JVM system properties of your running container. The Istio [hotspot=gateway file=0]`gateway` is the entry point for HTTP requests to the cluster. 
The `Host` header of your `system` service and `inventory` service to be `system.example.com` and `inventory.example.com`, respectively. 
You can set the `Host` header with the `-H` option of the `curl` command.

services.yaml
[source, yaml, linenums, role='code_column']
----
include::finish/services.yaml[]
----

Make a request to the service by using `curl`:

****
[system]#*{win} | {mac}*#


[role=command]
```
curl -H Host:inventory.example.com http://localhost/inventory/systems/system-service -I
```

If the `curl` command is unavailable, then use https://www.getpostman.com/[Postman^]. Postman enables you
to make requests using a graphical interface. To make a request with Postman, enter `\http://localhost/inventory/systems/system-service`
into the URL bar. Next, switch to the `Headers` tab and add a header with key of `Host` and value of `inventory.example.com`.
Finally, click the blue `Send` button to make the request.

[system]#*{linux}*#


[role=command]
```
curl -H Host:inventory.example.com http://`minikube ip`:31380/inventory/systems/system-service -I
```
****

You will see the following output:

[source, role="no_copy"]
----
HTTP/1.1 200 OK
x-powered-by: Servlet/4.0
content-type: application/json
date: Mon, 19 Aug 2019 19:49:47 GMT
content-language: en-US
x-envoy-upstream-service-time: 4242
server: istio-envoy
transfer-encoding: chunked
----

Since the `system` service is available, the request to the service is successful and returns a `200` response code.

To see the number of times that the `system` service was called, check the logs of the `system` pod using the `kubectl logs` command.
Remember to replace `[system-pod-name]` with the pod name associated with your `system` service.

****
[system]#*{mac} | {linux}*#


[role=command]
```
kubectl logs [system-pod-name] -c istio-proxy | grep -c system-service:9080
```

[system]#*{win}*#


[role=command]
```
kubectl logs [system-pod-name] -c istio-proxy | find /C "system-service:9080"
```
****

You will see the following output:

[source, role="no_copy"]
----
1
----

The above command returns 1, meaning that 1 request is made to the `system` service. 
Your single request to the service is successful with a `200` response code.

Now you will make the `system` service unavailable and MicroProfile's Retry policy will be performed.

Pause the `system` service pod to simulate that the service is unavailable. Replace `[system-pod-name]` with the pod name associated with
your `system` service, which you previously saw when running the `kubectl get pods` command.

[role=command]
```
kubectl exec -it [system-pod-name] /opt/ol/wlp/bin/server pause
```

You will see the following output:

[source, role="no_copy"]
----
Pausing the defaultServer server.
Pausing the defaultServer server completed.
----

Make a request to the service by using `curl`:

****
[system]#*{win} | {mac}*#


[role=command]
```
curl -H Host:inventory.example.com http://localhost/inventory/systems/system-service -I
```

If the `curl` command is unavailable, then use https://www.getpostman.com/[Postman^].

[system]#*{linux}*#


[role=command]
```
curl -H Host:inventory.example.com http://`minikube ip`:31380/inventory/systems/system-service -I
```
****


You will see the following output:

[source, role="no_copy"]
----
HTTP/1.1 500 Internal Server Error
x-powered-by: Servlet/4.0
content-type: text/html;charset=ISO-8859-1
$wsep:
content-language: en-US
date: Thu, 15 Aug 2019 13:21:57 GMT
x-envoy-upstream-service-time: 2929
server: istio-envoy
transfer-encoding: chunked
----

The request returns a `500` response code, as the `system` service is unavailable.
However, the request was retried several times with MicroProfile `@Retry` before failing.

See the number of times that the service was retried:

****
[system]#*{mac} | {linux}*#


[role=command]
```
kubectl logs [system-pod-name] -c istio-proxy | grep -c system-service:9080
```

[system]#*{win}*#


[role=command]
```
kubectl logs [system-pod-name] -c istio-proxy | find /C "system-service:9080"
```
****

You will see the following output:

[source, role="no_copy"]
----
13
----

The above command returns 13, since there were a total of 13 requests made to the `system` service.
These requests include Istio's default number of retries and the retries set by MicroProfile.

// =================================================================================================
// Enabling Istio fault tolerance
// =================================================================================================

== Enabling Istio fault tolerance

You previously implemented the Retry policy to retry requests to your `system` service using MicroProfile Fault Tolerance.
This Retry policy can also be implemented with Istio fault tolerance.

[role="code_command hotspot", subs="quotes"]
----
#Update the `traffic.yaml` file.#
`traffic.yaml`
----
traffic.yaml
[source, yaml, linenums, role='code_column']
----
include::finish/traffic.yaml[]
----
[role="edit_command_text"]
Add the [hotspot=istioRetry file=0]`retries` field in the [hotspot file=0]`traffic.yaml` file, to allow Istio to
retry requests a maximum of 4 times, when the request returns any `5xx` response code. 

The [hotspot=attempts file=0]`attempts` field is required in the configuration of Istio's Retry policy, and it indicates the
maximum number of retries that will be attempted for a given request. To retry a request on specific conditions, the
[hotspot=retryOn file=0]`retryOn` field is used. Since your paused `system` service responds with a `500` response code, 
you set [hotspot=retryOn file=0]`retryOn` to be `5xx`. 
Other https://www.envoyproxy.io/docs/envoy/latest/configuration/http_filters/router_filter#x-envoy-retry-on[retry conditions^] 
can also be specified in [hotspot=retryOn file=0]`retryOn`. An optional field that can be provided to Istio's Retry policy
is `perTryTimeout`, which indicates the amount of time that is allocated to each retry attempt. 

After integrating an Istio Retry policy into your microservices, deploy your microservices again, 
and wait until all of your deployments are ready and available:

[role=command]
```
kubectl replace --force -f services.yaml
kubectl replace --force -f traffic.yaml
```

Pause the `system` service pod to simulate that the service is unavailable:

[role=command]
```
kubectl exec -it [system-pod-name] /opt/ol/wlp/bin/server pause
```

Make a request to the service by using `curl`:

****
[system]#*{win} | {mac}*#


[role=command]
```
curl -H Host:inventory.example.com http://localhost/inventory/systems/system-service -I
```

If the `curl` command is unavailable, then use https://www.getpostman.com/[Postman^].

[system]#*{linux}*#


[role=command]
```
curl -H Host:inventory.example.com http://`minikube ip`:31380/inventory/systems/system-service -I
```
****

You will see that the request still returns a `500` response code, as the `system` service is unavailable.
This time however, the request was retried several times with Istio before failing.

See the number of times that the service was retried:

****
[system]#*{mac} | {linux}*#


[role=command]
```
kubectl logs [system-pod-name] -c istio-proxy | grep -c system-service:9080
```

[system]#*{win}*#


[role=command]
```
kubectl logs [system-pod-name] -c istio-proxy | find /C "system-service:9080"
```
****

You will see the following output:

[source, role="no_copy"]
----
60
----

The above command returns 60, indicating that a total of 60 requests are made to the `system` service.
These requests include Istio's default number of retries, the retries 
that you enabled in the [hotspot file=0]`traffic.yaml` file, and the retries set by MicroProfile.

You will disable some of MicroProfile's capabilities, including Retry, 
to have your `system` service retry with only Istio's Retry policy. 


// =================================================================================================
// Turning off MicroProfile Fault Tolerance except Fallback
// =================================================================================================

== Turning off MicroProfile Fault Tolerance except Fallback

When microservices have the same fault tolerance policy enabled by both MicroProfile and Istio, the behaviour of the policy is not as expected. 
If both MicroProfile and Istio set their own Retry policies on a service,
the maximum number of retries that occur is not equivalent to either of the number of retries specified in MicroProfile or Istio. 
The microservice actually multiplies the number of MicroProfile and Istio retries.

In the case that you want your microservices to use a service-mesh architecture, such as Istio, and use it's fault handling without interfering with
the service's MicroProfile capabilities, you can use a config property offered by MicroProfile.
      
=== Setting the MP_Fault_Tolerance_NonFallback_Enabled config property

MicroProfile Fault Tolerance offers a config property `MP_Fault_Tolerance_NonFallback_Enabled` that disables all 
MicroProfile Fault Tolerance capabilities except fallback. If `MP_Fault_Tolerance_NonFallback_Enabled` is set to false, all of the MicroProfile
annotations, including `@Retry`, will be switched off and only keep `@Fallback` available to use.

You will use the `MP_Fault_Tolerance_NonFallback_Enabled` config property to disable the retries performed by MicroProfile, 
to only allow Istio to perform the retries.

[role="code_command hotspot", subs="quotes"]
----
#Create the `config.yaml` file.#
`config.yaml`
----

config.yaml
[source, yaml, linenums, indent=0, role="code_column"]
----
include::finish/config.yaml[]
----

In the [hotspot file=0]`config.yaml`, the [hotspot=nonFallback file=0]`MP_Fault_Tolerance_NonFallback_Enabled` config property is set to false in a [hotspot=configMap file=0]`ConfigMap`. 
ConfigMaps store configuration settings about a Kubernetes pod. This configuration needs to be loaded into the pod as an environment variable to be 
used by the pod's containers. The environment variables are defined in the pod's specification using the `envFrom` field. 

[role="code_command hotspot", subs="quotes"]
----
#Update the `services.yaml` file.#
`services.yaml`
----
services.yaml
[source, yaml, linenums, role='code_column']
----
include::finish/services.yaml[]
----
[role="edit_command_text"]
Add the [hotspot=invConfig file=1]`envFrom` field in the [hotspot file=1]`services.yaml` file, 
to inject the [hotspot=configMap file=0]`ConfigMap` with the [hotspot=nonFallback file=0]`MP_Fault_Tolerance_NonFallback_Enabled` property into your pods. 



The [hotspot=configName file=0]`name` of the [hotspot=configMap file=0]`ConfigMap`, which is [hotspot=configName file=0]`inventory-config`, 
becomes the environment variable [hotspot=configName2 file=1]`name` that is specified in the [hotspot=invConfig file=1]`envFrom` field. 

Deploy your microservices again, and wait until all of your deployments are ready and available:

[role=command]
```
kubectl replace --force -f services.yaml
kubectl replace --force -f traffic.yaml
```

Turn off all MicroProfile capabilities, except fallback, by creating the [hotspot=configMap file=0]`ConfigMap` in [hotspot file=0]`config.yaml`:

[role=command]
```
kubectl apply -f config.yaml
```

Pause the `system` service pod to simulate that the service is unavailable:

[role=command]
```
kubectl exec -it [system-pod-name] /opt/ol/wlp/bin/server pause
```

Make a request to the service by using `curl`:

****
[system]#*{win} | {mac}*#


[role=command]
```
curl -H Host:inventory.example.com http://localhost/inventory/systems/system-service -I
```

If the `curl` command is unavailable, then use https://www.getpostman.com/[Postman^].

[system]#*{linux}*#


[role=command]
```
curl -H Host:inventory.example.com http://`minikube ip`:31380/inventory/systems/system-service -I
```
****

You will see that the request still returns a `500` response code, as the `system` service is unavailable.
This time however, the request was retried several times with Istio, without any retries from MicroProfile.

See the number of times that the service was retried:

****
[system]#*{mac} | {linux}*#


[role=command]
```
kubectl logs [system-pod-name] -c istio-proxy | grep -c system-service:9080
```

[system]#*{win}*#


[role=command]
```
kubectl logs [system-pod-name] -c istio-proxy | find /C "system-service:9080"
```
****

You will see the following output:

[source, role="no_copy"]
----
15
----

The above command returns 15, indicating that a total of 15 requests are made to the `system` service.
Since MicroProfile's Retry policy is disabled, only Istio's retries are performed.

traffic.yaml
[source, yaml, linenums, role='code_column']
----
include::finish/traffic.yaml[]
----

=== Adding the MicroProfile @Fallback annotation

config.yaml
[source, yaml, linenums, indent=0, role="code_column"]
----
include::finish/config.yaml[]
----

Since retrying the requests to your `system` service is still not successful, you need a plan to "fall back" on when the requests fail. 
You will create a fallback method to deal with the failed requests to your `system` service.

Although you disabled MicroProfile `@Retry` and other MicroProfile Fault Tolerance policies using the 
[hotspot=nonFallback file=0]`MP_Fault_Tolerance_NonFallback_Enabled` config property, the fallback policy is still available to use.
As mentioned before, Istio does not offer any fallback capabilities, and so the MicroProfile `@Fallback` annotation must be used 
to add fallback to your microservices.

The `@Fallback` annotation dictates a method to call when the original method encounters a failed execution. 
If your microservices have a Retry policy specified, then the fallback occurs after all of the retries have failed. 

[role="code_command hotspot", subs="quotes"]
----
#Update the `InventoryResource.java` file.#
`inventory/src/main/java/io/openliberty/guides/inventory/InventoryResource.java`
----
InventoryResource.java
[source, Java, linenums, role='code_column hide_tags=copyright']
----
include::finish/inventory/src/main/java/io/openliberty/guides/inventory/InventoryResource.java[]
----
[role="edit_command_text"]
Create the [hotspot=fallbackMethod file=1]`getPropertiesFallback()` method. 
Add the [hotspot=fallback file=1]`@Fallback` annotation before the [hotspot=getPropertiesForHost file=1]`getPropertiesForHost()` method, 
to call the [hotspot=fallbackMethod file=1]`getPropertiesFallback()` method when a failure occurs.



The [hotspot=fallbackMethod file=1]`getPropertiesFallback()` method, which is the designated fallback method for the original 
[hotspot=getPropertiesForHost file=1]`getPropertiesForHost()` method, prints out a warning message in the browser saying that 
the `system` service may not be running.

Rebuild your application and integrate the Fallback policy into your microservices:

[role=command]
```
mvn package
```

Deploy your microservices again, and wait until all of your deployments are ready and available:

[role=command]
```
kubectl replace --force -f services.yaml
kubectl replace --force -f traffic.yaml
```

Turn off all MicroProfile capabilities, except fallback, by creating the [hotspot=configMap file=0]`ConfigMap` in [hotspot file=0]`config.yaml`:

[role=command]
```
kubectl apply -f config.yaml
```

Pause the `system` service pod to simulate that the service is unavailable:

[role=command]
```
kubectl exec -it [system-pod-name] /opt/ol/wlp/bin/server pause
```

Make a request to the service by using `curl`:

****
[system]#*{win} | {mac}*#


[role=command]
```
curl -H Host:inventory.example.com http://localhost/inventory/systems/system-service -I
```

If the `curl` command is unavailable, then use https://www.getpostman.com/[Postman^].

[system]#*{linux}*#


[role=command]
```
curl -H Host:inventory.example.com http://`minikube ip`:31380/inventory/systems/system-service -I
```
****

You will see the following output:

[source, role="no_copy"]
----
HTTP/1.1 200 OK
x-powered-by: Servlet/4.0
x-from-fallback: true
content-type: application/json
date: Mon, 19 Aug 2019 19:49:47 GMT
content-language: en-US
x-envoy-upstream-service-time: 4242
server: istio-envoy
transfer-encoding: chunked
----

You can see that the request is now successful and returns a `200` response code, since the fallback method is called 
when the `system` service is not available.

See the number of times that the service is retried before the fallback method is called:

****
[system]#*{mac} | {linux}*#


[role=command]
```
kubectl logs [system-pod-name] -c istio-proxy | grep -c system-service:9080
```

[system]#*{win}*#


[role=command]
```
kubectl logs [system-pod-name] -c istio-proxy | find /C "system-service:9080"
```
****

You will see the following output:

[source, role="no_copy"]
----
3
----

The above command returns 3, indicating that a total of 3 requests are made to the `system` service.
The Istio retries that you enabled on the `inventory` service are not performed, since the Fallback policy is enabled. 
However, the 3 default requests by Istio on the server end are still performed. 
Since all of these requests failed, the fallback method [hotspot=fallbackMethod file=1]`getPropertiesFallback()` is called. 


// =================================================================================================
// Tearing down your environment
// =================================================================================================

== Tearing down your environment

You might want to teardown all the deployed resources as a cleanup step.

Delete your resources from the cluster.

[role=command]
```
kubectl delete -f services.yaml
kubectl delete -f traffic.yaml
kubectl delete -f config.yaml
```

Delete the `istio-injection` label from the default namespace. The hyphen immediately
after the label name indicates that the label should be deleted.

[role=command]
```
kubectl label namespace default istio-injection-
```

Navigate to the directory where you extracted {istio} and delete the {istio} resources from the cluster.

[role=command]
```
kubectl delete -f install/kubernetes/istio-demo.yaml
```

****
[system]#*{linux} | {mac}*#

[role=command]
```
for i in install/kubernetes/helm/istio-init/files/crd*yaml; do kubectl delete -f $i; done
```

[system]#*{win}*#
[role=command]
```
FOR %i in (install\kubernetes\helm\istio-init\files\crd*yaml) DO kubectl delete -f %i
```
****


[role=command]
include::{common-includes}/kube-minikube-teardown.adoc[]

// =================================================================================================
// finish
// =================================================================================================

== Great work! You're done!

You learned how to use the Retry policy to make your microservice more resilient to failures 
and how to build a fallback mechanism for your microservices.

// Include the below from the guides-common repo to tell users how they can contribute to the guide

include::{common-includes}/attribution.adoc[subs="attributes"]
